# %%
import datetime
import time
import yfinance as yf
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import numpy as np
import finta as TA
import matplotlib.pyplot as plt
import matplotlib.collections as collections
import matplotlib.dates as mdates
import os
import docx
from docx.shared import Inches
import gc
import sys



# list of interested tickers
TICKER_LIST = ["aapl","goog","ko","mmm","msft","nee","noc","ntr","pfe","pnc","pypl","sbac","slb"]

# url for nasdaq
NEWS_URL = "https://www.nasdaq.com/market-activity/stocks/{}/news-headlines"

# url for yahoo finance
ANALYST_URL = "https://finance.yahoo.com/quote/{}/analysis?p={}"

# pathname
mypath = os.getcwd()

# analysis periods
periods = [1,3,7,14,21,28]

# kinda lazy so set up a simple date conversion for month str to int
DATES = {
    "JAN":"01",
    "FEB":"02",
    "MAR":"03",
    "APR":"04",
    "MAY":"05",
    "JUN":"06",
    "JUL":"07",
    "AUG":"08",
    "SEP":"09",
    "OCT":"10",
    "NOV":"11",
    "DEC":"12"
}

DPI = 600

# make directory for figures if it isnt made
if os.path.isdir(mypath+'/figs'):
    pass
else:
    os.mkdir(mypath+'/figs')

class Stock:
    def __init__(self, ticker, market_time='Close') -> None:
        # initalize the given ticker name
        self.ticker = ticker
        self.market_time = market_time
        print("Initializing ticker {}".format(self.ticker))
        
        # initialize variable for random data to be stored
        self.data = {
            'Analysts':{
                
            },
            'Headlines':{
                
            },
            'TA':{
                
            }
        }
        
        # Check if dir exist, Otherwise make it
        if os.path.isdir(mypath+'/figs/{}'.format(self.ticker)):
            pass
        else:
            os.mkdir(mypath+'/figs/{}'.format(self.ticker))

        # use yfinance to get historical price/volume data
        print("Gathering price and volume history for {}".format(self.ticker))
        self.stock = yf.Ticker(self.ticker)
        self.price_history = self.stock.history(period="max")
        
        # Love new updates... this fixes localized datetime
        self.price_history.index = self.price_history.index.tz_localize(None)

        # FinTa requires the column names to be in lower case so recast the column names
        self.price_history_lower = self.price_history.copy()
        self.price_history_lower.columns = ["open", "high", "low", "close", "volume", "dividends", "stock splits"]

        # This is kinda deprecated due to its limited usage and replaced with scraping NASDAQ
        self.news_history = self.stock.news

        # attempt to run sentiment analysis. This may initalize scraping if headline file doesn't exist
        print("Querying historical headline data and running sentiment analysis")
        self.sentiment_analysis()
        
        # # attempt to get analyst recommendations
        print("Querying historical analysts' data")
        self.analyst_recommendation()

        # TAs will be autogenerated from the data at the start of each clas creation
        # TAs implemented are:
        '''
        RSI (30,70)
        OBV (grad-, grad+)
        DMI (DMI-, DMI+) ->
        ADX (25, 50, 75)
        SMA
        '''

        # Calculate RSI and set up bounds
        print("Calculating RSI")
        self.rsi = TA.TA.RSI(self.price_history_lower).to_frame()
        self.rsi = self.rsi.dropna()
        self.rsi["bounds"] = 0
        self.rsi.loc[self.rsi["14 period RSI"]>=70, "bounds"] = 1
        self.rsi.loc[self.rsi["14 period RSI"]<=30, "bounds"] = -1

        # Calculate OBV and set up bounds
        print("Calculating OBV")
        self.obv = TA.TA.OBV(self.price_history_lower).to_frame()
        self.obv = self.obv.dropna()
        self.obv["diff"] = self.obv["OBV"].diff()
        self.obv["bounds"] = 0
        self.obv = self.obv.dropna()
        self.obv.loc[self.obv["diff"] > 0, "bounds"] = 1
        self.obv.loc[self.obv["diff"] < 0, "bounds"] = -1

        # Calculate DMI and set up bounds
        print("Calculating DMI")
        self.dmi = TA.TA.DMI(self.price_history_lower)
        self.dmi = self.dmi.dropna()
        self.dmi["dmi_bounds"] = 0
        self.dmi.loc[self.dmi["DI+"] > self.dmi["DI-"], "dmi_bounds"] = 1
        self.dmi.loc[self.dmi["DI+"] < self.dmi["DI-"], "dmi_bounds"] = -1

        # Calculate ADX and set up bounds
        print("Calculating ADX")
        self.adx = TA.TA.ADX(self.price_history_lower).to_frame()
        self.adx = pd.merge(self.dmi, self.adx, 'outer', left_index=True, right_index=True)
        self.adx = self.adx.dropna()
        self.adx['bounds'] = 0
        self.adx.loc[(self.adx['14 period ADX.']>25) & (self.adx['dmi_bounds'] == 1) , 'bounds'] = 1
        self.adx.loc[(self.adx['14 period ADX.']>25) & (self.adx['dmi_bounds'] == -1) , 'bounds'] = -1
        self.adx.loc[(self.adx['14 period ADX.']>50) & (self.adx['dmi_bounds'] == 1) , 'bounds'] = 2
        self.adx.loc[(self.adx['14 period ADX.']>50) & (self.adx['dmi_bounds'] == -1) , 'bounds'] = -2
        self.adx.loc[(self.adx['14 period ADX.']>75) & (self.adx['dmi_bounds'] == 1) , 'bounds'] = 3
        self.adx.loc[(self.adx['14 period ADX.']>75) & (self.adx['dmi_bounds'] == -1) , 'bounds'] = -3
        
        # run analysis
        # analysis for analysts'
        print('Running accuracy analysis for analysts')
        self.analyst_accuracy()
        
        # run TA
        print('Running accuracy anaysis for TAs')
        self.TA_accuracy()
        
        print('Running accuracy analysis for headlines')
        self.headline_accuracy()
        
        print('Generating report')
        self.generate_report()

############### END of __init__ ##################################

############### Scraping Functions ###############################
    def scrape_news(self, save=True):
        # get current time to help identify the date of the article
        # this is mostly an edge case for articles written within 24 hrs
        now = datetime.datetime.now()
        url = NEWS_URL.format(self.ticker)

        # initialize selenium webdriver and open page
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
        driver.get(url)

        # try/except is mostly for my slow internet. It will keep trying 20 times or until it works
        # get the number of tabs to find out how many times to iterate
        attempts = 0
        while attempts < 20:
            try:
                time.sleep(5)
                total_tabs = driver.find_element(by=By.XPATH, value="/html/body/div[3]/div/main/div[2]/div[4]/div[3]/div/div[1]/div/div[1]/div[3]/div/button[8]").text
                break
            except:
                attempts += 1
        total_tabs = int(total_tabs)
        print("total tabs: {}".format(total_tabs))

        # set up csv column names
        self.headlines = [["Date", "Headline"]]

        # scrape data from NASDAQ
        for i in range(total_tabs):
            results = False
            attempts = 0
            while attempts < 20:
                try:
                    time.sleep(0.5)
                    print("scraping tab {}".format(i+1))
                    current_page_list = driver.find_element(by=By.XPATH, value="/html/body/div[3]/div/main/div[2]/div[4]/div[3]/div/div[1]/div/div[1]/ul")
                    options = current_page_list.find_elements(by=By. TAG_NAME, value="li")

                    for headline in options:
                        headline_text = headline.text.split("\n")
                        if "HOURS" in headline_text[0]:
                            post_time = now-datetime.timedelta(hours=int(headline_text[0].split(" ")[0]))
                            headline_text[0] = post_time.strftime("%m/%d/%Y")
                        elif "DAY" in headline_text[0]:
                            post_time = now-datetime.timedelta(days=int(headline_text[0].split(" ")[0]))
                            headline_text[0] = post_time.strftime("%m/%d/%Y")
                        elif "MIN" in headline_text[0]:
                            post_time = now-datetime.timedelta(minutes=int(headline_text[0].split(" ")[0]))
                            headline_text[0] = post_time.strftime("%m/%d/%Y")
                        elif "HOUR" in headline_text[0]:
                            post_time = now-datetime.timedelta(hours=int(headline_text[0].split(" ")[0]))
                            headline_text[0] = post_time.strftime("%m/%d/%Y")
                        else:
                            post_time = headline_text[0].split(" ")
                            post_time_full = str(DATES[post_time[0]])+"/"
                            post_time_full += str(post_time[1]).replace(",","")+"/"
                            post_time_full += str(post_time[2])
                            headline_text[0] = post_time_full

                        # reformat some weird headlines with extra \n
                        if len(headline_text) > 2:
                            temp_headline = headline_text[1]
                            for element in range(len(headline_text)-2):
                                temp_headline += " "+headline_text[i+2]
                            temp_date = headline_text[0]
                            headline_text = [temp_date, temp_headline]


                        
                        self.headlines.append(headline_text)
                    
                    break

                except:
                    attempts += 1

            # click for the next tab
            next_tab_button = driver.find_element(by=By.XPATH, value="/html/body/div[3]/div/main/div[2]/div[4]/div[3]/div/div[1]/div/div[1]/div[3]/button[2]")
            driver.execute_script("arguments[0].click();", next_tab_button)

        # save as a csv. The default is true
        
        df = pd.DataFrame(self.headlines[1:], columns=self.headlines[0])
        df.to_csv("headlines/{}.csv".format(self.ticker), index=False)
    
        # set data to self
        self.headlines = df
        return self.headlines
    
    def scrape_analyst(self):
        # scrape analyst reccomendations from yahoo finance's webpage
        self.analyst = [["Change","Analyst","Recommendation","Date"]]

        url = ANALYST_URL.format(self.ticker, self.ticker)

        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
        driver.get(url)

        attempt = 0
        while attempt < 20:
            try:
                time.sleep(0.25)
                attempt1 = 0
                while attempt1 < 3:
                    try:
                        time.sleep(0.25)
                        close_popup = driver.find_element(by=By.XPATH, value= '//*[@id="myLightboxContainer"]/section/button[1]')
                        driver.execute_script("arguments[0].click();", close_popup)
                        break
                            
                    except:
                        attempt1 += 1

                # scroll to the bottem because the site is picky
                driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")

                # get the open tab button, scroll to it, and click it
                open_tab_button = driver.find_element(by=By.XPATH, value='//*[@id="Col2-6-QuoteModule-Proxy"]/div/section/button')
                driver.execute_script("arguments[0].scrollIntoView(true);",open_tab_button)
                driver.execute_script("arguments[0].click();", open_tab_button)
                

                break

            except:
                attempt += 1

        
        attempt = 0
        while attempt < 20:
            try:
                time.sleep(0.5)
                table_body = driver.find_element(by=By.XPATH, value='//*[@id="Col2-6-QuoteModule-Proxy"]/div/section/table/tbody')
                recommendation = table_body.find_elements(by=By.TAG_NAME, value="tr")
                break

            except:
                attempt += 1

        for i in recommendation:
            to_append = []
            element = i.find_elements(by=By.TAG_NAME, value="td")
            to_append.append(element[1].text)

            element_company_rec = element[2].text.split(":")
            to_append.append(element_company_rec[0])
            to_append.append(element_company_rec[1])

            to_append.append(element[3].text)

            self.analyst.append(to_append)
        
        df = pd.DataFrame(self.analyst[1:], columns=self.analyst[0])
        df.Recommendation = df.Recommendation.str.strip()
        df.to_csv("analysts/{}.csv".format(self.ticker), index=False)

        self.analyst = df
        return self.analyst
    
############### END of Scraping Functions ###############################

############### Run Analysis on Stuff to get Bounds ###############################
    def sentiment_analysis(self):
        # check if headlines have been scraped
        try:
            self.headlines = pd.read_csv("headlines/{}.csv".format(self.ticker), index_col=None)
        except:
            self.scrape_news(True)
            self.headlines = pd.read_csv("headlines/{}.csv".format(self.ticker), index_col=None)

        # convert the date into a datetime format/variable/type
        self.headlines["Date"] = pd.to_datetime(self.headlines["Date"])
        self.headlines["Sentiment"] = np.nan
        sentiment = []

        # iterate through all of the headlines and score them
        for index, row in self.headlines.iterrows():
            sid = SentimentIntensityAnalyzer()
            ss = sid.polarity_scores(row["Headline"])
            sentiment.append(ss["compound"])
        
        self.headlines["Sentiment"] = sentiment
        
        self.data['Headlines']['Best_Headline'] = self.headlines[
            self.headlines.Sentiment == self.headlines.Sentiment.max()
        ]
        self.data['Headlines']['Worst_Headline'] = self.headlines[
            self.headlines.Sentiment == self.headlines.Sentiment.min()
        ]

        # group the headlines by date and then take the mean to find average score per day
        self.sentiment = self.headlines[["Date","Sentiment"]]
        self.sentiment = self.sentiment.groupby("Date").mean()
        
        self.sentiment.index = pd.to_datetime(self.sentiment.index)
        self.sentiment["Value"] = 0
        
        self.sentiment.loc[self.sentiment['Sentiment'] > 0, 'Value'] = 1
        self.sentiment.loc[self.sentiment['Sentiment'] < 0, 'Value'] = -1
        
        self.sentiment = pd.merge(self.sentiment, self.price_history[['High','Low','Close']], how='outer', left_index=True, right_index=True)
        self.sentiment.Value.iloc[0] = -999
        self.sentiment.Value = self.sentiment.Value.fillna(method='ffill')
        
        return self.sentiment

    def analyst_recommendation(self):
        try:
            self.analyst = pd.read_csv("analysts/{}.csv".format(self.ticker), index_col=None)

        except:
            self.scrape_analyst()

        self.analyst["Date"] = pd.to_datetime(self.analyst["Date"])
        self.analyst["Value"] = 0
        
        # quantify recommendations
        # maintaining
        self.analyst.loc[self.analyst["Recommendation"] == 'to Buy', 'Value'] = 1
        self.analyst.loc[self.analyst["Recommendation"] == 'to Outperform', 'Value'] = 1
        self.analyst.loc[self.analyst["Recommendation"] == 'to Overweight', 'Value'] = 1     
        
        self.analyst.loc[self.analyst["Recommendation"] == 'to Sell', 'Value'] = -1
        self.analyst.loc[self.analyst["Recommendation"] == 'to Underperform', 'Value'] = -1
        self.analyst.loc[self.analyst["Recommendation"] == 'to Underweight', 'Value'] = -1
        
        # change to 1
        self.analyst.loc[self.analyst["Recommendation"] == 'Hold to Buy', 'Value'] = 1
        self.analyst.loc[self.analyst["Recommendation"] == 'Peer Perform to Outperform', 'Value'] = 1
        self.analyst.loc[self.analyst["Recommendation"] == 'Neutral to Overweight', 'Value'] = 1
        self.analyst.loc[self.analyst["Recommendation"] == 'Equal-Weight to Overweight', 'Value'] = 1
        
        # change to 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Buy to Hold', 'Value'] = 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Outperform to Peer Perform', 'Value'] = 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Overweight to Neutral', 'Value'] = 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Overweight to Equal-Weight', 'Value'] = 0
        
        self.analyst.loc[self.analyst["Recommendation"] == 'Sell to Hold', 'Value'] = 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Underperform to Peer Perform', 'Value'] = 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Underweight to Neutral', 'Value'] = 0
        self.analyst.loc[self.analyst["Recommendation"] == 'Underweight to Equal-Weight', 'Value'] = 0
        
        # change to -1
        self.analyst.loc[self.analyst["Recommendation"] == 'Hold to Sell', 'Value'] = -1
        self.analyst.loc[self.analyst["Recommendation"] == 'Peer Perform to Underperform', 'Value'] = -1
        self.analyst.loc[self.analyst["Recommendation"] == 'Neutral to Underweight', 'Value'] = -1
        self.analyst.loc[self.analyst["Recommendation"] == 'Equal-Weight to Underweight', 'Value'] = -1
        
        self.analyst_stripped = self.analyst[['Date','Value']]
        self.analyst_stripped = self.analyst_stripped.groupby('Date').mean()
        self.analyst_stripped.loc[self.analyst_stripped.Value < 0, 'Value'] = -1
        self.analyst_stripped.loc[self.analyst_stripped.Value > 0, 'Value'] = 1
        
        self.analyst_stripped = pd.merge(self.analyst_stripped, self.price_history[['High','Low','Close']], how='outer', left_index=True, right_index=True)
        self.analyst_stripped.Value.iloc[0] = -999
        self.analyst_stripped.Value = self.analyst_stripped.Value.fillna(method='ffill')
        
        self.analyst_stripped['Value'] = pd.to_numeric(self.analyst_stripped['Value'], downcast='integer')

        return self.analyst_stripped

############### END of Analysis ###############################

############### Consider Updating over rescraping ###############################
    
    def update_headlines(self):
        # update the headlines without needing to rescrape the whole thing
        try:
            self.headlines = pd.read_csv("headlines/{}.csv".format(self.ticker), index_col=None)
        except:
            print('No local data to update')
            self.scrape_news(True)
            self.headlines = pd.read_csv("headlines/{}.csv".format(self.ticker), index_col=None)
        
        
            

            
        pass

############### Accuracy Functions (Pretty much plotting) ###############################

    def headline_accuracy(self):
        # check headline accuracy
        self.subplot_total(
            self.sentiment, 'Value',[-1,1,-999],[0.7,0.7,0.25],['r','g','grey'],
            'figs/{}/headline_accuracy_{}.png'.format(self.ticker,self.ticker),
            'Headline Analysis for {}'.format(self.ticker)
        )
        
        self.subplot_years(
            self.sentiment, 'Value',[-1,1,-999],[0.7,0.7,0.25],['r','g','grey'],
            'figs/{}/headline_accuracy_years_{}.png'.format(self.ticker,self.ticker),
            'Annual Headline Analysis for {}'.format(self.ticker)
        )
        
        self.data['Headlines']['Analysis'] = self.plotting_shift(
            self.sentiment, 'Value', [-1,1], ['Negative Headline Analysis','Positive Headline Analysis'],
            [
                'figs/{}/headline_accuracy_negative_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/headline_accuracy_positive_{}.png'.format(self.ticker,self.ticker),
            ]
        )
        
    def TA_accuracy(self):
        # check TA accuracy
        
        #############################
        # RSI
        self.subplot_total(
            self.rsi, 'bounds',[-1,1],[0.7,0.7],['r','g'],
            'figs/{}/RSI_accuracy_{}.png'.format(self.ticker,self.ticker),
            'RSI Analysis for {}'.format(self.ticker)
        )
        self.subplot_years(
            self.rsi, 'bounds',[-1,1],[0.7,0.7],['r','g'],
            'figs/{}/RSI_accuracy_years_{}.png'.format(self.ticker,self.ticker),
            'Annual RSI Analysis for {}'.format(self.ticker)
        )
        
        self.data['TA']['RSI'] = self.plotting_shift(
            self.rsi, 'bounds', [-1,1], ['Negative RSI Analysis','Positive RSI Analysis'],
            [
                'figs/{}/RSI_accuracy_negative_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/RSI_accuracy_positive_{}.png'.format(self.ticker,self.ticker),
            ]
        )
        
        #############################
        # OBV
        self.subplot_total(
            self.obv, 'bounds',[-1,1],[0.7,0.7],['r','g'],
            'figs/{}/OBV_accuracy_{}.png'.format(self.ticker,self.ticker),
            'OBV Analysis for {}'.format(self.ticker)
        )
        
        self.subplot_years(
            self.obv, 'bounds',[-1,1],[0.7,0.7],['r','g'],
            'figs/{}/OBV_accuracy_years_{}.png'.format(self.ticker,self.ticker),
            'Annual OBV Analysis for {}'.format(self.ticker)
        )
        self.data['TA']['OBV'] = self.plotting_shift(
            self.obv, 'bounds', [-1,1], ['Negative OBV Analysis','Positive OBV Analysis'],
            [
                'figs/{}/OBV_accuracy_negative_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/OBV_accuracy_positive_{}.png'.format(self.ticker,self.ticker),
            ]
        )
        
        #############################
        # ADX
        self.subplot_total(
            self.adx, 'bounds',[-3,-2,-1,1,2,3],[0.8,0.6,0.4,0.4,0.6,0.8],['r','r','r','g','g','g'],
            'figs/{}/ADX_accuracy_{}.png'.format(self.ticker,self.ticker),
            'ADX Analysis for {}'.format(self.ticker)
        )
        
        self.subplot_years(
            self.adx, 'bounds',[-3,-2,-1,1,2,3],[0.8,0.6,0.4,0.4,0.6,0.8],['r','r','r','g','g','g'],
            'figs/{}/ADX_accuracy_years_{}.png'.format(self.ticker,self.ticker),
            'Annual ADX Analysis for {}'.format(self.ticker)
        )
        
        self.data['TA']['ADX'] = self.plotting_shift(
            self.adx, 'bounds', [-3,-2,-1,1,2,3], 
            ['Negative ADX Analysis: -3','Negative ADX Analysis: -2','Negative ADX Analysis: -1',
             'Positive ADX Analysis: 1','Positive ADX Analysis: 2','Positive ADX Analysis: 3'],
            [
                'figs/{}/ADX_accuracy_negative_-3_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/ADX_accuracy_negative_-2_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/ADX_accuracy_negative_-1_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/ADX_accuracy_positive_1_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/ADX_accuracy_positive_2_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/ADX_accuracy_positive_3_{}.png'.format(self.ticker,self.ticker),
            ]
        )
        

    def analyst_accuracy(self):
        # check analyst accuracy
        self.subplot_total(
            self.analyst_stripped, 'Value',[-1,1,-999],[0.7,0.7,0.25],['r','g','grey'],
            'figs/{}/analyst_accuracy_{}.png'.format(self.ticker,self.ticker),
            'Analyst Analysis for {}'.format(self.ticker)
        )
        
        self.subplot_years(
            self.analyst_stripped, 'Value',[-1,1,-999],[0.7,0.7,0.25],['r','g','grey'],
            'figs/{}/analyst_accuracy_years_{}.png'.format(self.ticker,self.ticker),
            'Annual Analyst Analysis for {}'.format(self.ticker)
        )
        
        self.data['Analysts']['Analysis'] = self.plotting_shift(
            self.analyst_stripped, 'Value', [-1,1], ['Negative Analyst Analysis','Positive Analyst Analysis'],
            [
                'figs/{}/analyst_accuracy_negative_{}.png'.format(self.ticker,self.ticker),
                'figs/{}/analyst_accuracy_positive_{}.png'.format(self.ticker,self.ticker),
            ]
        )

############### END of Accuracy Functions ###############################   

    def subplot_years(self, df, bounds_name, bounds_values, alpha_values, color, name, title):
        beginning = df.index[0].year
        end = df.index[-1].year
        
        # capped to fit on a word page
        total_rows = 10
        
        columns = int(round(((end-beginning)/total_rows)+0.5))
        rows = int(round(((end-beginning)/columns)+0.5))
        fig, ax = plt.subplots(rows,columns)
        if (end-beginning) < total_rows:
            for year in range(end-beginning):
                ax[year].plot(self.price_history[self.market_time].loc[str(year+beginning):str(year+beginning)])
                df_slice = df[str(year+beginning):str(year+beginning)]
                inxval = mdates.date2num(df_slice.index.to_pydatetime())
                for num in range(len(bounds_values)):
                    collection = collections.BrokenBarHCollection.span_where(
                        inxval, 0,
                        self.price_history.High.loc[str(year+beginning):str(year+beginning)].max(), df_slice[bounds_name] == bounds_values[num],
                        facecolor=color[num], alpha=alpha_values[num]
                    )
                    ax[year].add_collection(collection)
                    ax[year].tick_params(labelrotation=45)
                    
        else:
            count = 0
            for col in range(columns):
                for r in range(rows):
                    ax[r][col].plot(self.price_history[self.market_time].loc[str(count+beginning):str(count+beginning)])
                    df_slice = df[str(count+beginning):str(count+beginning)]
                    inxval = mdates.date2num(df_slice.index.to_pydatetime())
                    for num in range(len(bounds_values)):
                        collection = collections.BrokenBarHCollection.span_where(
                            inxval, 0,
                            self.price_history.High.loc[str(count+beginning):str(count+beginning)].max(), df_slice[bounds_name] == bounds_values[num],
                            facecolor=color[num], alpha=alpha_values[num]
                        )
                        ax[r][col].add_collection(collection)
                        ax[r][col].tick_params(labelrotation=45)
                    count += 1
                    if count >(end-beginning):
                        break
        
        if columns == 1:
            fig.set_size_inches(6.5,(end-beginning))
        else:
            fig.set_size_inches(6.5,1.5*(end-beginning)/columns)
        fig.set_dpi(DPI*columns)
        fig.suptitle(title)
        fig.tight_layout(pad=1)
        plt.close(fig)
        fig.savefig(name)
        # fig.show()
            
    def subplot_total(self, df, bounds_name, bounds_values, 
                      alpha_values, color, name, title):
        fig, ax = plt.subplots()
        inxval = mdates.date2num(df.index.to_pydatetime())
        ax.plot(self.price_history[self.market_time])
        for num in range(len(bounds_values)):
            collection = collections.BrokenBarHCollection.span_where(
                            inxval, 0,
                            self.price_history.High.max(), df[bounds_name] == bounds_values[num],
                            facecolor=color[num], alpha=alpha_values[num]
            )
            ax.add_collection(collection)
        
        fig.set_dpi(DPI)
        fig.suptitle(title)
        fig.tight_layout(pad=1)
        plt.close(fig)
        fig.savefig(name)
        # fig.show()
        
    def plotting_shift(self, df, bounds_name, bounds_values,title,path):
        df_temp = df[bounds_name]
        df_temp = pd.merge(df_temp, self.price_history[['High','Low','Close']], how='outer', left_index=True, right_index=True)
        df_temp = df_temp.dropna()
        for days in periods:
            df_temp['{}-day'.format(days)] = df_temp[self.market_time].shift(-days) - df_temp[self.market_time]
        df_temp = df_temp.dropna()
        to_return = {}
        for i in range(len(bounds_values)):
            fig, ax = plt.subplots(sharex=True)
            fig.tight_layout()
            fig.set_size_inches(6.5,9)
            fig.set_dpi(DPI*2)
            df_temp[['1-day','3-day','7-day','14-day','21-day','28-day']].loc[
                df_temp[bounds_name]==bounds_values[i]].plot(
                kind='line',
                subplots=True,
                lw=0.5,
                title=title[i],
                legend=True,
                ax=ax
            )
            plt.close(fig)
            fig.savefig(path[i])
            
            to_return[str(bounds_values[i])+'_mean'] = df_temp[['1-day','3-day','7-day','14-day','21-day','28-day']].loc[
                df_temp[bounds_name]==bounds_values[i]].mean()
            to_return[str(bounds_values[i])+'_min'] = df_temp[['1-day','3-day','7-day','14-day','21-day','28-day']].loc[
                df_temp[bounds_name]==bounds_values[i]].min()
            to_return[str(bounds_values[i])+'_max'] = df_temp[['1-day','3-day','7-day','14-day','21-day','28-day']].loc[
                df_temp[bounds_name]==bounds_values[i]].max()
            
        return to_return
    
    def unwrap_data(self, document, bounds, group, subgroup):
        num_rows = len(bounds)*3
        unpacked_data = [self.data[group][subgroup]['{}_min'.format(bounds[0])].index.tolist()]
        table = document.add_table(1,num_rows+1)
        table.style = 'LightShading-Accent1'
        heading_cells = table.rows[0].cells
        heading_cells[0].text = 'Timeframe'
        for i in range(len(bounds)):
            heading_cells[int(i*3+1)].text = '{}_min'.format(bounds[i])
            heading_cells[int(i*3+2)].text = '{}_mean'.format(bounds[i])
            heading_cells[int(i*3+3)].text = '{}_max'.format(bounds[i])
            
            unpacked_data.append(self.data[group][subgroup]['{}_min'.format(bounds[i])].tolist())
            unpacked_data.append(self.data[group][subgroup]['{}_mean'.format(bounds[i])].tolist())
            unpacked_data.append(self.data[group][subgroup]['{}_max'.format(bounds[i])].tolist())
        
        for i in range(len(unpacked_data[0])):
            cells = table.add_row().cells
            for j in range(len(unpacked_data)):
                if type(unpacked_data[j][i]) == str:
                    cells[j].text = unpacked_data[j][i]
                else:
                    try:
                        cells[j].text = str(round(unpacked_data[j][i],2))
                    except:
                        cells[j].text = 'NaN'
                
        return table
        
        
    
############### Generate Report ###############################

    def generate_report(self):
        # generate a report of the data
        document = docx.Document()
        document.add_heading('Automatice Report for {}'.format(self.ticker), 0)
        
        # Begin by talking about headline stuff
        document.add_heading('Analyzing the Headline Accuracy', 1)
        document.add_picture('figs/{}/Headline_accuracy_{}.png'.format(self.ticker,self.ticker))
        document.add_picture('figs/{}/Headline_accuracy_years_{}.png'.format(self.ticker,self.ticker),height=Inches(9))
        
        document.add_heading('')
        
        document.add_heading('A Little Bit More Insight Into the Headlines')
        document.add_paragraph(
            'The best headline was"{}" which occured on {} and had a Sentiment Score of {}'.format(
                self.data['Headlines']['Best_Headline']['Headline'].values[0],
                self.data['Headlines']['Best_Headline']['Date'].to_string().split(' ')[-1],
                self.data['Headlines']['Best_Headline']['Sentiment'].values[0],
            )
        )
        document.add_paragraph(
            'The worst headline was"{}" which occured on {} and had a Sentiment Score of {}'.format(
                self.data['Headlines']['Worst_Headline']['Headline'].values[0],
                self.data['Headlines']['Worst_Headline']['Date'].to_string().split(' ')[-1],
                self.data['Headlines']['Worst_Headline']['Sentiment'].values[0],
            )
        )
        
        self.unwrap_data(document,[-1,1],'Headlines','Analysis')
        
        document.add_page_break()
        
        # Next start talking about Technical Indicator stuff
        document.add_heading('Analyzing the Technical Indicators', 1)
        document.add_heading('Looking at RSI', 2)
        document.add_picture('figs/{}/RSI_accuracy_{}.png'.format(self.ticker,self.ticker))
        document.add_picture('figs/{}/RSI_accuracy_years_{}.png'.format(self.ticker,self.ticker),height=Inches(9))
        
        self.unwrap_data(document,[-1,1],'TA','RSI')
        
        document.add_heading('Looking at OBV', 2)
        document.add_picture('figs/{}/OBV_accuracy_{}.png'.format(self.ticker,self.ticker))
        document.add_picture('figs/{}/OBV_accuracy_years_{}.png'.format(self.ticker,self.ticker),height=Inches(9))
        self.unwrap_data(document,[-1,1],'TA','OBV')
        
        document.add_heading('Looking at ADX', 2)
        document.add_picture('figs/{}/ADX_accuracy_{}.png'.format(self.ticker,self.ticker))
        document.add_picture('figs/{}/ADX_accuracy_years_{}.png'.format(self.ticker,self.ticker),height=Inches(9))
        self.unwrap_data(document,[-2,-1,1,2],'TA','ADX')
        
        document.add_page_break()
        
        # Finally end by talking about Analyst Agency stuff
        document.add_heading('Analyzing the Analyst Agencies', 1)
        document.add_picture('figs/{}/analyst_accuracy_{}.png'.format(self.ticker,self.ticker))
        document.add_picture('figs/{}/analyst_accuracy_years_{}.png'.format(self.ticker,self.ticker),height=Inches(9))
        self.unwrap_data(document,[-1,1],'Analysts','Analysis')
        
        
        # Save the document
        document.save('reports/{}.docx'.format(self.ticker))

# %%

if __name__ == "__main__":
    www = Stock('www')
    # for s in TICKER_LIST:
    #     t = Stock(s)
        
    #     # delete variable otherwise computer dies
    #     del t
    #     gc.collect()